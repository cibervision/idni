- title: The Art of Self-Reference
  key: main
  link: the-art-of-self-reference
  author: Ohad Asor
  date: May 27, 2018
  content: This post, like the previous two, comes to shed light on the same subjects. The post "The New Tau" went bottom up from TML to Agoras and focused on scaling discussions.  The post "From Agoras to TML" went the other way around, and focused on knowledge economy. Here we approach the subject by emphasizing on self definition.
  main_content: <p>This post, like the previous two, comes to shed light on the same subjects. The post "The New Tau" went bottom up from TML to Agoras and focused on scaling discussions.  The post "From Agoras to TML" went the other way around, and focused on knowledge economy. Here we approach the subject by emphasizing on self definition.</p>
                <p>Our main example here would be of legislation. Consider a parliament which creates and changes laws. Suppose the parliament is interested in changing an existing law.  Well, in a normal world, they can't just go and change the law, as there should be laws of changing the laws.  Distinguishing between laws and laws of changing laws, we refer to the formers as "first order laws" and to the latters a "second order laws". A third order law would be a law that regards second order laws (and possibly first order as well).</p>
                <p>As it looks, we will need infinitely many orders of laws, therefore infinitely many laws, just in order not to leave the law completely unprotected, if there are no laws of changing the laws (second order laws), then there's absolutly nothing preventing from changing first order laws in any case.  And that's not enough, if some second order law is preventing some law change, we can also change that second order law, as long as we don't have a third order law that prevents it.</p>
                <p>So according to this approach, just in order to have any kind of protection against arbitrary law change, we got to have infinitely many laws. In technical terms, it means that no high order logic would be enough, as any order of formula will still be finite, and the maximum order of a formula will correspond to the maximum order of a law, which would then be completely unprotected against modification, as above. This shows how any trial to solve the problem using high order logic or type theory is doomed to fail, unless they incorporate the following remedy.</p>
                <p>And the remedy is to consider recursive rules. Infinite order rules, as shown, are a necessity of everyday life.  Consider the law, "all laws can be changed only upon majority vote, including this law". What would be the order of this law? If you say that it's of order is N, then I can say that it's of order N+1 really, because it refers to itself, and a law that refers to an order N law is by definition of order at least N+1. So, the order of this law, is really infinite or undefined (can go either way). It refers to itself, and creates a "logical infinite loop".</p>
                <p>The reader might be familiar with self-referencing statements in forms of paradoxes, like the liar paradox, or simply "this statement is false". Self-referencing definitions may not only be contradictory but also meaningless, "the green-colored things are those who have green color", defines "green-colored things", but in a circular definition that gives us no information at all.  Nevertheless, our example law "all laws can be changed only upon majority vote, including this law" makes a perfect sense. It refers to itself, but there's no problem with that, we can still understand and follow this law. It would be ridiculus to reject this law just because we cannot attach to it any finite order in the sense of high order logic.</p>
                <p>Definitions by self-reference appear very frequently.  We can define a number X by "X is the number such that X=4X-6". It defines X by means of itself, yet a perfectly valid way to say X=2. In programming, it is about recursive functions, functions that call themselves, directly or indirectly. However naively programming this definition would immediately fail.</p>
                <p>"function what_is_x() return 4*what_is_x() - 6"</p>
                <p>this function will run forever, and will not admit our common sense that X=4X-6 is a simple valid finite definition.</p>
                <p>So we really need to be able to define things in terms of themselves, and we can see how common programming and logic paradigms fail to do so.</p>
                <p>The family of logics that incorporate self-reference as a language primitive is called "Fixed-Point Logics".  Self-reference, recursion, fixed point, are more or less synonyms in the field of computer science. There is an urban legend saying that the use of recursion automatically implies undecidability and by that uselessness in many scopes including the scope of law. This is probably because another way to define Turing machines is using "Recursively Enumerable Sets", tying generic computation with recursion. However this is not always the case. There exist decidable fixed-point logics, for example FO[LFP] and FO[PFP] (both are decidable logics because they operate over finite structures).  We won't get into the details now, I'll just mention that TML is FO[PFP].</p>
                <p>(Decidability very roughly means, for the reader unfamiliar with the concept yet, that it is possible to answer all relevant questions. A too expressive language will become undecidable because it will require many [if not most] of the questions to need infinite time to answer).  </p>
                <p>We have pointed to a logical framework that makes sense for legislation and showed how other logical frameworks break down. Similarly we showed that common programming paradigms also break down on these aspects, and our alternative paradigm is viable for programming languages as well. To require the law to be written in a decidable fixed-point logic, is a highly nontrivial and highly specific requirement, but I think that we demonstrated how there isn't really any other way to go.</p>
                <p>But of course even though it eliminates so many other possibilities, it is still not enough. It basically amounts to the kind of language of law, to allow recursion and keep it decidable. Still, a lot more things has to be said about such a language and the use of it, to be adequate for legislation. This has to do with the field called KRR (Knowledge Representation and Reasoning). I don't have too much to say about it, we basically adopt the ontological model together with the relational machine model. For more information about knowledge representation as ontologies and relations, can refer to resources about the Semantic Web.</p>
                <p>So we move next from language to knowledge to how knowledge is acquired, in a social setting? Well, it is never the case that people simply write down all their opinions. Opinions are communicated by (and also arised from) events and discussions. More deeply, opinions arise from interest, or even deeper, from questions.</p>
                <p>And now we reach to speak about the most important thing in life, the questions. We have the notion of a correct answer. We can even program computers to tell whether a given answer to a given question, is correct. We are interested in correct answers to our questions. But what would be the kind of questions we are interested at? Well, we are interested in interesting questions, yet another circular definition that happen to be meaningless. Which questions are interesting?</p>
                <p>While the answer to the question "for which X we have X=4X-6" to be X=2 is correct to any intelligent being, to anyone who can understand the question. The information in this specific question is enough to objectively deem the answer X=2 as correct, no matter humans, machines, or aliens. Still, it contains absolutly no information regarding why would this be interesting, if any.</p>
                <p>Questions being interesting, is not only subjective, but inherently always stem from the arbitrary preferences of the asker. A dog might ask to dig and a cat might ask a mat. There isn't such a thing "an interesting question", but "a question interesting for certain beings in certain time". Our questions come from our human nature and our personal nature. We are defined by the questions we find interesting, so much more than we are defined by the answers we give. Similarly, cooperation between people interested in the same questions, makes much more sense comparing to cooperation between people agreeing on the same answers. And many would be much more interested in finding people asking same questions as them, than finding people giving same answers. Questions are therefore a major aspect of tau, including the points raised and beyond (for example the role of questions in the setting of open world assumption vs the closed world assumption).</p>
                <p>For clarity we can distinguish between "questions" and "queries". By queries we refer to questions in which we expect to have an immediate answer, e.g. the case where we feed a machine with information, and then query that information. The machine will not and should not return new information, it will only use the infromation that we gave to it.  In contrast, by "questions" we refer to questions in which we don't expect to have an available answer yet. A question is a tool to define which knowledge is desired. Questions usually come before the knowledge, not the other way around.  They are a tool to scope a discussion or an exploration into certain areas of knowledge. A tool which the machine will never be able to simulate, but given the human input of which questions are interesting, it can greatly help us in finding correct answers by discussions about those questions.</p>
                <p>We can now conclude with the role of the people vs the role of the machine, on tau, humans are for questions, and machines are for answers. More broadly, I see this as a philosophical truth that should guide any AI aspirations.</p>
                <p>From our six steps of language, knowledge, discussion, collaboration, choice, and knowledge economy, we now covered how in order to reach the goal of social choice that may refer to the process of social choice itself, we need very specific kinds of formalisms which goes down to the language level (without covering the internet of languages from previous posts), and from there we touched some additional apects of knowledge and discussion.</p>
                <p>Next time, Who gets to decide? </p>

- title: From Agoras to TML
  link: from-agoras-to-tml
  key: main
  author: Ohad Asor
  date: Mar 13, 2018
  content: In the last blogpost we went through our five stages in chronological and conceptually bottom-up order language, knowledge, discussion, collaboration, change, and knowledge market. The first five steps are about creation of a knowledge society, and from there we can build a knowledge economy on top of it. On this post we go backwards we consider what we mean by knowledge economy and what is required for that to happen, and go down the hierarchy until language.
  main_content: <p>In the last blogpost we went through our five stages in chronological and conceptually bottom-up order, language, knowledge, discussion, collaboration, change, and knowledge market. The first five steps are about creation of a knowledge society, and from there we can build a knowledge economy on top of it. On this post we go backwards, we consider what we mean by knowledge economy and what is required for that to happen, and go down the hierarchy until language.</p>
                <p>To make the ideas more concrete, let's consider a decentralized search engine, which was one of Agoras' goals that was later generalized into the concept of knowledge economy. Google has ~1M physical servers which need to crawl, index, and search the internet. I don't know how big their data is, but it must be huge. So having the whole internet indexed and to actually search it, one needs enormous amounts of computing power and data storage. In a decentralized network featuring a decentralized search engine, who is going to pay for that?</p>
                <p>Since this task of maintaining a web search engine depends on "physical" data, on data that is completely unknown until someone goes to the internet and discovers it. Therefore, such a task cannot be completely trustless, as one cannot prove that they downloaded and indexed into the search engine the correct data, and didn't modify, replace, or omit it. But although this problem isn't completely solvable, it is still solvable to some probabilistic extent, with risks that can be lowered arbitrarily, theoretically. We will not cover this whole subject now, but we did on Zennet's materials, cf. e.g. http://zennet.sc/about or more documents in Zennet's bitcointalk thread. My pricing formula appears at http://zennet.sc/zennetpricing.pdf which eliminates the risk of mispricing, a risk that can be significantly exploited. Additional way to probabilistically verify unverifiable computations is to calculate the same thing more than once (by randomly choosing more providers), so increasing the cost linearly, decreases the risk exponentially (e.g. x10 more cost yields ^10 less risk).</p>
                <p>So having a decentralized search engine requires Zennet-like capabilities, the ability to fairly rent (and rent-out) computational resources, under acceptable risk in the user's terms (as a function of cost). Our knowledge market will surely require such capabilities, and is therefore one of the three main ingredients of Agoras. But let's continue, say we have hardware rent market, what's next towards a decentralized search engine?</p>
                <p>A web search engine consists of users and maintainers. The users supply search queries, and the maintainers answer the queries quickly, and to do so quickly they must have the whole web already indexed and stored. Naturally, users have to pay to maintainers a payment that depends on the amount of usage and the cost of maintenance.</p>
                <p>But in a decentralized network, the users and the maintainers are really the same entity. To maintain the network all one has to do is to run the client and by that participate in indexing and searching. A user might query a certain amount of queries per day, while their computer can answer another certain amount of queries per day (to other users as, no single user will store the whole internet, while you can't predict what you're going to query tomorrow). The computer, can probably answer orders of magnitudes more queries than a single average user can manually provide. So a home user who uses dozens or hundreds of "googlings" per day, and also runs a client that supports the network, I'd expect such a user not only to not have to pay, but to earn, as they'll serve others more than they consume. But we will also have heavier users, such that for example consumes the search services automatically in large magnitudes. Such users will have to pay and will probably not end up break even. It's a zero-sum game, but the money flows from the big entities to the smaller ones.</p>
                <p>OK so we described a system that one can type a query and "ask the internet", and the engine returns an answer, together with an economic system that supports the production, pricing, shipment, and arena (agora), of some knowledge (the knowledge of the text on the web). Now it begins to sound as an example of a knowledge economy.</p>
                <p>If stopping here, we didn't actually contribute much, so we'll have a better and decentralized search engine that gives many users some income, but life will continue more or less the same. Searching, as we know it, is no different that searching a web page with Ctrl-F together with a thesaurus. That's what Google is more or less using, it Ctrl-F's the internet with an open thesaurus. But making an economy out of knowledge has many more possibilities. Importantly, we'd like to incorporate into our knowledge economy also deeper and more meaningful knowledge than "those words or their equivalents are mentioned on that website".</p>
                <p>Ultimately, we'd like to upload many (if not most) of our thoughts, opinions, and intellect, to the internet. We already do it. But all our search engines know to do is to use Ctrl-F and a thesaurus. namely it operates in a very shallow level, not even "shallow understanding". But knowledge that people actually seek for, is never this way. We don't seek for documents that contain certain words, but for documents that actually answer our question. Similarly, we don't seek for professionals in which all they know is to search the internet, but when we seek for professionals we expect from them a deep understanding of their subject of expertise, not by their ability to mention a bag of words. Otherwise we'd do good enough with Google and wouldn't need professionals anymore.</p>
                <p>And we'd like to have a futuristic electronic knowledge economy out of this all. To shed a broader light on the picture (and leaving aside for this post the whole monetary side of the economy, the monetary features that Agoras will support), an economy cannot exist without a society, and is a thing within society. Unsurprisingly, all economists consider themselves as a kind of social scientists. Unlike physics, economics depends, and cannot be defined without, concepts that exist purely in human's imagination. The principle that yields this status of economics is simple but very deep, we cannot have an economy without some subjective valuation (or "utility function"), namely the ability to say "I prefer this over that". Can think of it like taste, some prefer chocolate over ice cream, while some crazies prefer ice cream over chocolate. They might even be ready to pay different prices. In the bottom line, it comes down to ethical value systems in the broad sense, defining good and bad, better and worse. Which are indeed not physical terms but exist purely on our imagination. And this doesn't make them a single bit less important.</p>
                <p>Let me give a (really) pale example of a knowledge-economical feature that can be made over Agoras thanks to its Tau underlying infrastructure, giving it access to meaningful knowledge. Suppose there's some institute or faculty of philosophy that took the hard mission to formalize whole philosophy books in a tau-comprehensible language. They might offer you to submit questions and get answers that rely on their expensive proprietary database, for payment. Then can just declare this data, over the platform, to be private, and to incorporate it into certain discussions only upon paid subscription.</p>
                <p>Each one of us is a research institute of something, featuring unique knowledge. Such knowledge should have direct economic value, namely a pure knowledge-cash transaction.</p>
                <p>Alright so we need also a "knowledge society" to be the framework that our "knowledge economy" lives at. We will touch several aspects in which it is required, but let's follow our route and speak about the ability to change. An economy, is a game, and it's highly undesirable to not be able to change the rules of the game over time, which is again a social choice and highly depends on ethical values. Only society decides the new economical (and social) rules, as otherwise it's technically not going to happen. But what does it mean to "change the economy" or "change the society"?</p>
                <p>It means a lot, but we don't care. We are computer people. Our knowledge society and economy, Tau and Agoras, are computer programs. So "what does it mean to change economy/society" comes to a very clear practical concept, a decentralized network that changes its own code.</p>
                <p>To change its own code, as it sounds, is a sensitive thing to do, both by practice and by design, both from technical reasons and social reasons. The old tau considered only the technical side (and even that in a wrong way), and neglected the social side. Indeed there are questions that we don't know to give an apriori answer as there is no best answer, for example, who can change the rules (in a decentralized setting)? There is no perfect answer to this question, and every solution suffers from major disadvantages, yet, as there is no other way, the users will have to decide which partial solution they see as less worse. So the "governance model" will ultimately have to be decided by the users, and that's one of the benefits of beginning the discussion platform centralized, to reach a broad and machine-assisted consensus about the nature of the decentralized network since its beginning.</p>
                <p>But aside "who gets to change the rules" there are many, many social aspects in the process of change that require attention and do have good solutions. For some taste of those aspects please see the previous blogpost (for example "who gets to vote" vs "who gets to decide what to vote over", a question that becomes meaningless given our vote-less solution).</p>
                <p>A self amending core, is the Tau, as also explained in the latter post. After we have the ability to change, we can proceed to an evolving electronic knowledge economy. So we covered two steps on our way down, from knowledge market to change, namely from Agoras to Tau. Let's keep going down backwards through collaboration (beta), discussion and knowledge (alpha), and language (TML). But we'll do it in short.</p>
                <p>Collaboration, as "change" above, means a lot, but we don't care as we're computer people. To collaborate means to do something together, and the maximum that we can do in the computer world, is to run a program. Any p2p network is a form of collaboration, coordinated execution of code. On Tau we'll use the language translation abilities of TML in order to convert specification of programs to code of programs, automatically, and by that building and running a program together reduces to discussions, enhanced with the ability to convert logic to code, run it, and coordinate its execution among parties.</p>
                <p>From here to discussions. In order to agree which code to run (or even organize the opinions out of a large groups), and which new rules to set, and to actually have an electronic knowledge society, and to formalize knowledge in a human-convenient process, and to have this all effectively in a very large scale (millions or more of users), we need discussions that scale. And for this topic I have no choice but refer you again to the last blogpost.</p>
                <p>We reached down our road to the fundamental concepts of knowledge itself, and language that is even more fundamental than knowledge. Our knowledge society/economy can be fruitful more than as in the existing world, only if the machine have access to the meaning of things said, and not just to how they're written. We create the Internet of Languages, by letting users define new knowledge-representation languages (that might be very close to natural languages), and by that making an ever growing classes of documents to become machine comprehensible (let alone an ever growing number of documents). TML and the internet of languages will widely support more kinds languages than just knowledge representation, but that'd be enough for now.</p>
                <p>On this post we emphasized more on the top of the pyramid and less on its bottom, in contrary to the previous post. I hope it was somehow helpful. Thanks for reading!</p>
                


- title: The New Tau
  link: the-new-tau
  key: main
  author: Ohad Asor
  date: Dec 31, 2017
  content: We are interested in a process in which a small or very large group of people repeatedly reach and follow agreements. We refer to such processes as Social Choice. We identify five aspects arising from them language, knowledge, discussion, collaboration, and choice about choice. We propose a social choice mechanism by a careful consideration of these aspects.
  main_content: <p>We are interested in a process in which a small or very large group of people repeatedly reach and follow agreements. We refer to such processes as Social Choice. We identify five aspects arising from them, language, knowledge, discussion, collaboration, and choice about choice. We propose a social choice mechanism by a careful consideration of these aspects.</p>
                <p>Some of the main problems with collaborative decision making have to do with scales and limits that affect flow and processing of information. Those limits are so believed to be inherent in reality such that they're mostly not considered to possibly be overcomed. For example, we naturally consider the case in which everyone has a right to vote, but what about the case in which everyone has an equal right to propose what to vote over?</p>
                <p>In small groups and everyday life we usually don't vote but express our opinions, sometimes discuss them, and the agreement or disagreement or opinions map arises from the situation. But on large communities, like a country, we can only think of everyone having a right to vote to some limited number of proposals. We reach those few proposals using hierarchical (rather decentralized) processes, in the good case, in which everyone has some right to propose but the opinions flow through certain pipes and reach the voting stage almost empty from the vast information gathered in the process. Yet, we don't even dare to imagine an equal right to propose just like an equal right to vote, for everyone, in a way that can actually work. Indeed how can that work, how can a voter go over equally-weighted one million proposals every day?</p>
                <p>All known methods of discussions so far suffer from very poor scaling. Twice more participants is rarely twice the information gain, and when the group is too big (even few dozens), twice more participants may even reduce the overall gain into half and below, not just to not improve it times two.</p>
                <p>It turns out that under certain assumptions we can reach truly efficiently scaling discussions and information flow, where 10,000 people are actually 100 times more effective than 100 people, in terms of collaborative decision making and collaborative theory formation. But for this we'll need the aid of machines, and we'll also need to help them to help us.</p>
                <p>Specifically, the price is using only certain languages, that may still evolve with time, and by that letting computers be able to understand what we speak about, to understand things said during the discussions. Since no one knows how to make computers understand natural languages, we'll have to make a step towards machines and use machine comprehensible languages. We'll detail more about this point, but before let's speak a little about self-amendment.</p>
                <p>We describe a decentralized computer network, Tau Chain, and as such, which social decisions may it support? The most that computers can do is to run programs. Over Tau Chain we can gather knowledge and agree or disagree over it, and we can also actually do something and perform actions as arises from the discussion over the platform. Those actions, are nothing but computer programs. And the most important program on our scope is the platform itself.</p>
                <p>The main choices collaboratively be made over the system, are about the system itself. Tau, is a discussion about Tau. Or in a little more elaborate yet succinct definition:</p>
                <p style="padding:0 10%" class="text-center"><b>Consider a process, denoted by X, of people, forming and following another process denoted by Y. Tau is the case where X=Y.</b></p>
                <p>That's the Tau. What the Tau is doesn't matter, what matters is that it can change into anything we want it to be. Further, Tau is a computer program, so we refer to a program that changes itself up to the collaborative opinions and decisions of its users.</p>
                <p>It should be remarked that we do not let Tau guess the people's opinion, or even perform well-educated guesses as in machine learning, and that's maybe the main reason we use logic. Things said over the platform are as formal and definite as computer programs, they just deal with generic knowledge rather machine instructions.</p>
                <p>Having that, a collaboratively self-amending program, it can transform into virtually any program we'd like it to be, or many programs at once. Indeed Tau does not speak only about itself but open for creation of any other individual or collaborative activities, such that we make it possible for small and very large groups to discuss, share and organize knowledge, detect consensus and disagreements, and coordinate actions in forms of programs.</p>
                <p>The five aspects in social mentioned on the beginning correspond to the roadmap of Tau. Here's a brief summary to be emphasized more in the rest of this post. Implementation of TML and the internet of languages is the first step. Then comes the Alpha which is a discussion platform. Then the Beta which is about collaboratively following processes (not just defining them), specifically, it's about not just knowledge but also programs. Alpha and Beta are not fully decentralized in their infrastructure as in Bitcoin. Afterwards, and with the help of the Alpha and the Beta, comes Tau which is a decentralized self-amending social choice platform. On top of it we'll have a knowledge market which is one of Agoras' three components (the other two is computational resources market like Zennet, and newly designed economy offering features like risk free interest without printing new money, by implementing a derivatives market).</p>
                <p>In order for machines to boost our discussion and collaboration abilities, they have to have access to the meaning of what we say. Machines use certain kinds of languages while humans use different kinds. For machines to use human languages, is something no one knows how to do, and for humans to directly use machine languages is pretty much inconvenient to the extent that it simply doesn't fit common knowledge-sharing human communication, machine languages are made of machine instructions, while knowledge representation is of a different nature. In another words, machines expect operational information, while humans make a lot of use in declarative language. Indeed one of Tau's goals is to let us focus on the "know-what" and let machines figure out the "know-how".</p>
                <p>We therefore suggest a widely previously-suggested (cf. e.g. the article "Knowledge Representation and Classical Logic" by Lifschitz et al) place in the middle between human and machine languages, which is logic. Formal logic is largely natural to humans and is something machines can work with. But still "formal logic" isn't anything particular, as it doesn't point to any language but is a vague description of a family of languages.</p>
                <p>We postulate that there should not and can not be a single universal language. There is no reason for one language to be optimal (or even adequate) for all needs. We therefore come up with a meta-language that is able to define new languages, but hey this would be back to square one with one universal [meta-]language. We therefore require the meta-language to be able to redefine itself and change, just as it can define other languages. By that we get not only many languages but also a self-amending language, which is an important part in a self-amending system.</p>
                <p>It turns out that logics that can define themselves and have nice logical properties like decidability are not very common. We have Universal Turing Machines, but a less expressive and more informative (e.g. decidable) language is not easy to find. We adopt the logic PFP which its expressiveness is PSPACE-complete as known from Finite Model Theory books, and shown to be able to define itself in Imhof, 1999 "Logics that define their own semantics".</p>
                <p>From here we continue to the Internet of Languages. Using the meta-language which we call TML (Tau Meta-Language, can get impression from the ongoing work on github) users define new languages by specifying logical formulas to describe what it means for two documents in different languages to have same meaning. In other words, to define a new language, one needs to define how it translates a semantics-preserving translation into an existing language. Semantics in our scope is ontological (objects and relations), and not operational semantics as in programming languages. By that we get an internet of knowledge representation languages that make the choice of language to not matter. A document in one language can be routed (using TML programs) into different languages.</p>
                <p>We do not refer to translation as in French to Chinese, as we already stressed that we don't deal with natural languages. Of course, theoretically, it might be the case that one day someone will program over TML something that can understand natural language completely, but we don't count on such an event. Indeed there are many formalisms of natural language that are quite close to the full language and comfortable for humans to work with (what we refer to "simple enough English that machines can understand"), so we can expect TML to process human-comprehensible languages to some extent. But TML is intended also for machine-only languages. For example one might want to convert a document into a formatted HTML or into a Wiki, or to convert a program in some high-level language to machine code, or to synthesize code from logic.</p>
                <p>More generally, TML is intended to be a compiler-compiler. In order to be so efficiently and not having to consider the logic of the language[s] again and again with every compilation of documents written in it, we take the approach of Partial Evaluation, which gives rise to additional very desirable features for a compiler-compiler, in the form of Futamura projections.</p>
                <p>Now that we can express knowledge and opinions in various languages (precisely those languages that users define over the internet of languages over time), we can communicate using those languages. We consider Human-Human communication, or more specifically Human-Machine-Human communication. The machine is not an equal part in the conversation, it is only a machine, it only organizes what we say and is able to do so since we encode our information in a way accessible to it. A user can broadcast an idea to another user, and at this narrow scope of transmitting one idea between two people we can already enjoy from three benefits, easy explaining, easy understanding, and formalizing knowledge as a byproduct.</p>
                <p>Specifically, the explainer doesn't need to make the other user understand, they only need to make the machine understand. This task might be simpler on some aspects and more complex on others, still machines are certainly less bound to organization and scale than humans. Having achieved an idea formalized in a machine comprehensible language, the second user can now not only translate it to other knowledge representation languages or to organize it as they see fit or to compare it to other formalized ideas, they can also ask the machine all questions they got. Since the machine understood the subject completely, and by understood we indeed refer to the theoretical ability to answer all questions (decidability arises again here), it can help the user understand by the same definition of understanding, as it can answer all the user's questions, without the need to refer the question to the original idea's author.</p>
                <p>But the Alpha is beyond such case. The Alpha is about discussions of any scale. It is structured as discussions just like on forums or social networks, with posts and comments, that can appear in a team or a profile. A profile (or identity) is a place where people will typically post their personal opinions, and will be able to share them with other profiles they're connected with. A team is a group of identities, created and configured by some user, and intended to deal with a certain subject. For example a team could collaboratively develop of a software product, or compose an agreed law or contract, or simply any scientific/philosophical/social/nonsense ideas.</p>
                <p>So far sounds just like any other discussion platform, but here we can have many more features thanks to using machine comprehensible languages. To list a few, automatically detect repeated argument by same person, or collecting what each person said during the discussion and map all the agreement and disagreement points, or to list all opinions and then who agrees with them rather (speakers per opinion rather opinions per speaker), or to organize the information put on the discussion in more organized and readable forms like a wiki. It can even comment automatically, suppose you see a post by someone expressing some opinion, but you already expressed in length your opinion about the subject in the past. You could then click "autocomment" and the system will automatically express your opinion, based on the exact information you provided in the past, and relative to the post you're autocommenting to. Or, maybe most importantly, to calculate the set of statements agreed by everyone with no exception, under some scope in concern, can be network wide, or per team, or per profiles connected to my profile, oe per discussion, and so on. Remember, this is not a magic at all, once everything is written in logic (or given we have a logic that can translate it into logic, namely TML definitions of the documents' languages).</p>
                <p>Over the Alpha we teach the network a lot of knowledge, intentionally or as a byproduct of discussions. We also form theories that we agree on and all contributed to. What can we do with this knowledge? Ultimately, in the computer world, all we can do is run computer programs. On the Beta we will be able to discuss programs, and then actually run them. On Tau, we'll have a special team called Tau, such that whenever the group accepts a new decision, Tau's code is automatically modified. Over the Beta we can make true those things that we agreed as desirable over the Alpha, on our discussions. Once a team agrees or modifies its agreement on a specification of some program, no code need to be written or rewritten as it can be done automatically, as everything is already in machine-comprehensible languages. Synthesizing code from specifications is yet another language translation to be done over the internet of languages, but of course adequate language transformers have to be developed over it in order to allow such. This is a good example of things that are more easily said than done, and the details are highly technical. It is suffice to mention that the cutting-edge synthesis capabilities appear in the MSO+λY world.</p>
                <p>Choice about choice, is to choose how to choose. To be able to change the choice mechanism itself, or in another words, the rules of changing the rules, or equivalently, to change Tau's code with time. This as for itself raises paradoxes and constraints the possible logics. If rules can change themselves, they inevitably contradict themselves as they try to say something else. How can we formalize such process in a paradox-free manner? One may be misled to identify rules about rules and choice about choice with higher order logic, but this isn't enough. Consider for example the rule "all rules, including this, can be modified only upon majority". Since this rule operates on itself as well, it has no finite order. We therefore need recursion in order to deal with rules of changing the rules. This is an important aspect involved in the choice of fixed-point logic for TML, and λY calculus on the Beta (apropos, Bauer showed in "On Self-Interpreters For System-T and Other Typed λ-Calculi" that a language can self-interpret only if it has fixed point, which rules out total programming languages).</p>
                <p>An approach for rule-changing that was considered on the old Tau is Nomic's approach. To explain Nomic's approach and the new Tau's approach we'll use an example. Consider two lawyers each representing two sides of some deal, trying to converge into a contract such that both lawyers agree on. One way would be the following. First lawyer suggests a clause in the contract, and the second lawyer may agree or not. If agreed, then the clause is appended, otherwise it isn't. Then it's the second lawyer's turn to propose a clause and so on. This would be the Nomic way. The equivalent for Tau is to apply successive code patches with time. By that we pose an asymmetry between opinions that came first. There's a lot to say about this asymmetry and how Tau manages to avoid it almost completely, but for now, consider the case where a newly proposed clause contradicts an old clause. If we don't want to give priority to what came first, they will then have to amend the new or old clause or even more clauses, and not by default delete the old clause.</p>
                <p>Another way would be that on every turn, each lawyer submits a whole contract draft, and the other lawyer may either accept it or propose a different draft. Requiring each draft to be logically consistent, we will never have to deal with contradictions of past vs future. It eliminates completely the need to look back. But it still cannot scale. What if we had a million lawyers, will they read a million drafts?</p>
                <p>Over Tau we can take all those million contract drafts, which correspond to proposals of Tau's next full code, and in a quite straight-forward way (thanks to the logical formalism of the documents) calculate the precise core that everyone agree on, and list the points to be resolved. We don't need to vote, we do it just as in small groups in real life, we just speak, and the opinions map arise from the conversation to any intelligent listener.</p>
                <p>So much more to be said, and will be said in further blogposts and other publications, but that's all for now. I'll be more than happy for your opinions and approaches regarding the mentioned issues, especially practical social choice and how to make discussions scale.</p>
                

- title: "Tau and the Crisis of Truth"
  link: tau-and-the-crisis-of-truth
  key: main
  author: Ohad Asor
  date: Sep 11, 2016
  content: We live in a world in which no one knows the law. Except trivial cases, you cannot know what is legal and what is not, all you can do is try and see what is the judge’s or policeman’s opinion after you've taken your actions, which of course differs from one to another.
  main_content: <p>We live in a world in which no one knows the law. Except trivial cases, you cannot know what is legal and what is not, all you can do is try and see what is the judge’s or policeman’s opinion after you've taken your actions, which of course differs from one to another. Or you can consult a lawyer that will tell you that there are no absolute answers, and at the end of the day it’s a matter of probability which unfortunately no one knows how to calculate. You can do your best to have a productive life and follow the rules as you understand them or as lawyers guide you, but no one can guarantee that you will not be considered a criminal, or that legal actions will not be taken against you. Similarly one can live a life of harming so many people and no legal system will stop it even if the system is aware to the actions taken. Such pessimistic situation is not new and is not local, and to my taste was best described by Franz Kafka.</p>
                <p>Playing with words and taking them into any desired direction, consciously or not, with good or bad intentions, was always there since humanity learned to speak. The worst lies contain only truths and the worst crimes are licensed, and arguments can be given to justify almost anything. This “crisis of truth” is the foundation of the post-modern stream in philosophy, notably the <a target="_blank" href="https://en.wikipedia.org/wiki/Deconstruction">Deconstructivist</a> approach which demonstrates how texts may be interpreted in many (contradicting) ways. "There is no one truth" is the basis of post-modernism. But can we at least have some island of truth in which social contracts can be useful and make sense?</p>
                <p>Not only laws cannot be understood in an absolute way, but cannot even  trivially be made by an absolute process. Laws have to be changed with time and therefore we also need laws of changing the laws. But then we’ll need laws-of-changing-the-laws-of-changing-the-laws, ad infinitum. We therefore remain without any logical basis for the process of rulemaking, not only the crisis of deciding what is legal and what is illegal. This paradox was described on the online-available book <a target="_blank" href="http://legacy.earlham.edu/~peters/writing/psa/index.htm">The Paradox of Self Amendment</a> by Peter Suber. Suber’s solution is summarized on the appendix of that book that describes the game Nomic. He offers a two-tier system in which we have mutable and immutable rules, including rules of transmuting a rule (namely to make immutable rule mutable or vice versa). This way one can avoid the infinite hierarchy of amendment-schemes, but we still remain with the “crisis of truth” in which there is no one truth, especially when you desperately need it, and despite the very purpose of laws to have a certain social order rather disinformation and chaos.</p>
                <p>When stepping back from the world of legalities and trying to answer similar questions in the world of numbers and mathematical objects, the task doesn’t become easy but arguably easier. Logicians have contemplated those ideas for centuries and came up with the notion of Decidability. A language is decidable if every expressible statement on this language can be decided whether it follows from rules expressed in that language as well. An astonishing example would be Godel’s theorem. If our language is able to express <a target="_blank" href="https://en.wikipedia.org/wiki/Peano_axioms">Peano arithmetic</a>, namely, it contains the natural numbers 1,2,3,... equipped with addition and multiplication and their rules (that indeed determine them uniquely), then there will always be some true statements about Peano arithmetic that cannot be proved from the definitions. Infinitely many axioms will therefore be required in order to be able to decide the correctness of every statement. An example of decidability would be <a target="_blank" href="https://en.wikipedia.org/wiki/Presburger_arithmetic">Presburger arithmetic</a>. If we give up multiplication and define numbers with addition only (in a certain way), we are then back to decidability and every expressible statement could be decided in a finite process to be either true or false. Same applies to the 2000 years old Euclid geometry which is a decidable theory indeed.</p>
                <p>We would therefore be interested in creating a social process in which we express laws in a decidable language only, and collaboratively form amendable social contracts without diving into paradoxes. This is what Tau-Chain is about.</p>
                <p>This long introduction came to stress the importance and central role of decidability in generally in life and specifically in Tau. If we would like Tau to give a real contribution to the world, it must use a decidable language. The initial Tau design was aiming to answer those requirements, but recently I’ve found out that I was wrong, and the initially chosen language is simply undecidable. <a target="_blank" href="https://en.wikipedia.org/wiki/Intuitionistic_type_theory">Martin-Lof Type Theory</a> (MLTT) has only certain decidable aspects (specifically, equality). But it is not the case that everything expressible is decidable. This situation led me to a conclusion that MLTT was a wrong choice, and another logic should be considered. Indeed some statements of mine on blogposts and videos were mistaken, and MLTT does not have decidable typecheck in general.</p>
                <p>MLTT is about so-called “dependent types” which I’ll try to give a small taste of in this paragraph. We have “terms” and “types”. Can see terms as statements and types as specifications that those statements are required to fulfill. How general can be the specifications? Dependent types comes to give them maximum expressiveness in some sense, types can depend on terms, and terms can depend on types (and that’s where the name “dependent types” comes from). We are able to express any specification that our term language is able to express, or more accurately, types can be stated by a program that calculates them, that may depend on other parts of the code. Programs may include specifications, and specifications may include programs. Typecheck is the process of making sure that terms indeed meet their types. But if types can be generic programs, we have to require them to be executed and return a result in a finite time. For that we’ll have to make sure that our programs halt, so we have reached the <a target="blank" href="https://en.wikipedia.org/wiki/Halting_problem">Halting problem</a>. Only if you can assert that your types are calculated in a finite process, then the very process of typecheck becomes decidable. But this task is undecidable by itself as the Halting problem is undecidable. So if you want to reach decidability over dependent types, you’ll first have to pass an undecidable barrier of halting. This is possible in many cases as many programs are able to be written in a form that provably terminates (e.g. primitive recursion which is the basis of total functional programming), but it is inherently impossible to come up with such a method for all programs, otherwise the Halting problem would become decidable.</p>
                <p>Dependently typed languages are widely used in the world of computer-aided proofs and formal verification, but they have a reputation of being hard to work with. The hardness stems not only from their theory being significantly more complicated than of other languages, but the harder part is convincing the typechecker (or termination and totality checker that try to assure that the typecheck process is finite indeed) that your code meets its specifications. The typecheck process is therefore not automatic, and requires a significant amount of human intervention just in order to convince it to accept true statements, a difficulty that stems exactly from the undecidability of the language. Subsequently we lose the ability to always and fully automatically determine whether a sentence is true, or equivalently, if some action (term) is legal (well-typed) or illegal according to given rules.</p>
                <p>The remedy for this situation is to pick a different logic, a “complete” one. A complete logic means that everything is decidable, and not only certain aspects of it (like equality in MLTT). Godel showed that every logic that is complete and is able to express Peano arithmetic, will inevitably be inconsistent, so in order to enjoy both consistency and completeness we'll have to give up a part of arithmetic. Not only there are already known complete and consistent logics we can use, but very recently such families of languages were discovered to be much more expressive than previously known. Specifically, certain classes of Monadic Second Order Logic (MSO or MSOL) were found to be complete and consistent since the 1960s, but they did not have satisfactory expressiveness abilities. It was not until 2006 that Luke Ong came up with a proof of completeness of MSOL over so-called “Higher Order Recursion Schemes” which is a simply-typed (rather dependently-typed) higher order functional language with recursion, that dramatically enhanced the expressiveness of known completely decidable languages. Since then advancements in both theory and practice were made, notably by Naoki Kobayashi.</p>
                <p>It was a long journey until I found out that my basic premises were wrong and indeed I was not a logician when the journey began. Fortunately we can now modify Tau’s design into something that is even better than we thought before, even if MLTT was decidable. To mention three advantages, the existence of a direct computational model, the ability to infer programs, and simplicity of use.</p>
                <p>MSO theories have a direct computational model in terms of automatons. MSO formulas are able to be translated into redundant machines that read a statement and decide whether it meets the formula. Moreover, those automatons can be minimized in a unique way, so we even get a provably optimal code. MLTT on the other hand does not enjoy a direct translation to automatons but rather the logic-automata connection it admits a more general concept of programs-as-proofs, where the proof process is left more loosely specified in computational terms and a direct translation to automatons does not exist. It cannot possibly exist since automatons are an explicit finite decision procedure, while dependent types are not completely decidable.</p>
                <p>The second notable advantage is automatic programming, more commonly called “program synthesis” in the literature. If we give a specification of a program (or rule base), can the machine calculate the program (or rules) by itself? This task is undecidable on MLTT, but in a complete logic it is possible indeed. We can have correct-by-construction programs and contracts, by merely stating what we expect them to do, and commanding the machine to find such a structure that admits our requirements.</p>
                <p>MSO logic is simpler to use for the reasons mentioned above. It is a simpler language, and does not require human intervention to convince that a true statement is true indeed. It also represents a more intuitive logic. MLTT is an intuitionistic logic which means that a statement that proved to be false doesn't necessarily imply that it's contrast is true. For example, one cannot always use a proof by contradiction. Intuitionistic logic allows to prove nonexistence by contradiction. It can be assumed that something exists, derive a contradiction, and by that conclude it does not exist. But it cannot support a proof by contradiction of nonexistence, assuming that something does not exist and deriving a contradiction does not prove that it exists, under intuitionistic logic. More formally, intuitionistic logic explicitly negates the law of excluded middle. And this is of course highly unintuitive, and allows the possibility of human mistakes especially over complicated structures. On the other hand, MSO is a Classical logic, means it includes the law of excluded middle. It's simply impossible for anything to be both true and false at the same time. The middle is completely excluded, while if we add this rule to MLTT, it loses its consistency.</p>
                <p>As a conclusion, our journey turned out to be different than we expected from the aspect of implementation, but it will end up with a better product that will give us a chance to solve the crisis of truth and the paradox of self amendment in a global social and electronic context. I personally intend (and promised) to keep working full time until I fulfill Tau and Agoras that will introduce many more features than described here. Recently I gave a filmed condensed description of the products I’m committed to deliver. Another article and another video are on their way to explain more details. If you are willing to help achieving these goals and you find yourself (or your friend) understanding Kobayashi's papers, I'll be very glad for cooperation.</p>


- title: "Project Roadmap"
  link: project-roadmap
  key: archive
  author: Ohad Asor
  date: Apr 9, 2016
  content: Tau is a language. It is a programming language, but not only such. It may express virtually any arbitrary knowledge, rules, and processes. Examples are contracts, mathematical proofs, scientific facts and arguments, and legal documents.

- title: "Tau as a Generalized Blockchain"
  link: tau-as-a-generalized-blockchain
  key: archive
  author: Ohad Asor
  date: Oct 17, 2015
  content: Bitcoin may abstractly be described as a decentralized machine that appends new data (transactions) to a public shared database (ledger) given certain proofs (signatures).


- title: "Proof of Code Execution"
  link: proof-of-code-execution
  author: Ohad Asor
  key: archive
  date: Sep 6, 2015
  content: Putting it altogether, code is rules and queries, while the compiler (being essentially an autoprover) follows the rules and their consequences and outputs answers (while performing any side-effects).

- title: Decentralized Democracy and the Role of the First Users
  link: decentralized-democracy-and-the-role-of-the-first-users
  author: Ohad Asor
  key: archive
  date: Sep 1, 2015
  content: Recently we mentioned several times the logical and technological properties of tau, and now maybe it is time to take a step back and look at the broader picture. We mentioned that tau's programming language is actually rules and logic. 

